{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asasas\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\"\"\"Train and test engines\n",
    "\"\"\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n",
    "\n",
    "def train_fn(data_loader, model, optimizer, scheduler, device):\n",
    "\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    final_outputs = []\n",
    "    final_targets = []\n",
    "\n",
    "    for idx, data in tqdm(enumerate(data_loader), total = len(data_loader)):\n",
    "\n",
    "        ids = data['ids']\n",
    "        mask = data['mask']\n",
    "        target = data['target']\n",
    "\n",
    "        ids = ids.to(device, dtype = torch.long)\n",
    "        mask = mask.to(device, dtype = torch.long)\n",
    "        target = target.to(device, dtype = torch.float)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            ids, \n",
    "            token_type_ids = None, \n",
    "            attention_mask = mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        loss = loss_fn(logits, target)\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "\n",
    "        final_targets.extend(target.cpu().detach().tolist())\n",
    "        final_outputs.extend(torch.sigmoid(logits).cpu().detach().flatten().tolist())\n",
    "\n",
    "        # print(len(final_targets), len(final_outputs))\n",
    "    \n",
    "    return np.array(final_outputs), final_targets, total_train_loss / len(data_loader)\n",
    "\n",
    "\n",
    "def eval_fn(data_loader, model, device):\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    final_outputs = []\n",
    "    final_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, data in tqdm(enumerate(data_loader), total = len(data_loader)):\n",
    "            ids = data['ids']\n",
    "            mask = data['mask']\n",
    "            target = data['target']\n",
    "\n",
    "            ids = ids.to(device, dtype = torch.long)\n",
    "            mask = mask.to(device, dtype = torch.long)\n",
    "            target = target.to(device, dtype = torch.float)\n",
    "            \n",
    "            outputs = model(\n",
    "                ids, \n",
    "                token_type_ids = None, \n",
    "                attention_mask = mask, \n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            loss = loss_fn(logits, target)\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            final_targets.extend(target.cpu().detach().tolist())\n",
    "            final_outputs.extend(torch.sigmoid(logits).cpu().detach().flatten().tolist())\n",
    "\n",
    "    return np.array(final_outputs), final_targets, total_val_loss / len(data_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8068<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c7518ab8bb496385a4714daacef603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>wandb\\run-20211028_143001-3cplypmt\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>wandb\\run-20211028_143001-3cplypmt\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Train loss</td><td>0.48635</td></tr><tr><td>Epoch</td><td>0</td></tr><tr><td>_step</td><td>1</td></tr><tr><td>_runtime</td><td>220</td></tr><tr><td>_timestamp</td><td>1635411821</td></tr><tr><td>Valid loss</td><td>0.44647</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Train loss</td><td>▁</td></tr><tr><td>Epoch</td><td>▁▁</td></tr><tr><td>_step</td><td>▁█</td></tr><tr><td>_runtime</td><td>▁▁</td></tr><tr><td>_timestamp</td><td>▁▁</td></tr><tr><td>Valid loss</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">leafy-glitter-46</strong>: <a href=\"https://wandb.ai/likith012/uncategorized/runs/3cplypmt\" target=\"_blank\">https://wandb.ai/likith012/uncategorized/runs/3cplypmt</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: wandb version 0.12.6 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.8<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">hopeful-music-47</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/likith012/uncategorized\" target=\"_blank\">https://wandb.ai/likith012/uncategorized</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/likith012/uncategorized/runs/f5as6fbr\" target=\"_blank\">https://wandb.ai/likith012/uncategorized/runs/f5as6fbr</a><br/>\n",
       "                Run data is saved locally in <code>wandb\\run-20211028_143543-f5as6fbr</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/428 [00:00<?, ?it/s]C:\\Users\\likit\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 428/428 [03:16<00:00,  2.18it/s]\n",
      "100%|██████████| 107/107 [00:13<00:00,  7.91it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "wandb.init()\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(config.TRAINING_FILE, delimiter = '\\t', header = None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "\n",
    "df_train, df_valid = model_selection.train_test_split(\n",
    "    df,\n",
    "    test_size = 0.2,\n",
    "    random_state = 42,\n",
    "    stratify = df.label.values\n",
    ")\n",
    "\n",
    "# Drop indices\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_valid = df_valid.reset_index(drop=True)\n",
    "\n",
    "train_dataset = dataset.BERTDataset(\n",
    "    sentence = df_train.sentence.values,\n",
    "    target = df_train.label.values\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = config.TRAIN_BATCH_SIZE,\n",
    "    sampler = torch.utils.data.RandomSampler(train_dataset)\n",
    ")\n",
    "\n",
    "valid_dataset = dataset.BERTDataset(\n",
    "    sentence = df_valid.sentence.values,\n",
    "    target = df_valid.label.values\n",
    ")\n",
    "\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size = config.VALID_BATCH_SIZE,\n",
    "    sampler = torch.utils.data.SequentialSampler(valid_dataset)\n",
    ")\n",
    "\n",
    "model = config.MODEL.to(config.DEVICE)\n",
    "\n",
    "param_list = list(model.named_parameters()) # list of model parameters\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight'] # bias of hidden layer and layer norm are not decayed\n",
    "optimizer_parameters = [ # removing the no_decay params\n",
    "    {'params': [param for name, param in param_list if not any(nd in name for nd in no_decay)], 'weight_deacy': 0.001}, # without no_decay params\n",
    "    {'params': [param for name, param in param_list if any(nd in name for nd in no_decay)], 'weight_deacy': 0.} # only no_decay params\n",
    "]\n",
    "\n",
    "num_train_steps = int((len(df_train) / config.TRAIN_BATCH_SIZE) * config.EPOCHS)\n",
    "\n",
    "optimizer = AdamW(optimizer_parameters, lr = 3e-5, eps = 1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup( # gets called in train_fn as this scheduler is independant on valid loss\n",
    "    optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = num_train_steps\n",
    ")\n",
    "\n",
    "num_device = torch.cuda.device_count()\n",
    "device_ids = list(range(num_device))\n",
    "if len(device_ids) > 1:\n",
    "    model = nn.DataParallel(model, device_ids=device_ids) # distributed training\n",
    "\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(config.EPOCHS):\n",
    "\n",
    "    train_outputs, train_targets, train_loss = train_fn(train_dataloader, model, optimizer, scheduler, config.DEVICE)\n",
    "    valid_outputs, valid_targets, valid_loss = eval_fn(valid_dataloader, model, config.DEVICE)\n",
    "\n",
    "    wandb.log({'Train loss': train_loss, 'Epoch': epoch})\n",
    "    wandb.log({'Valid loss': valid_loss, 'Epoch': epoch})\n",
    "\n",
    "    tr = train_outputs[train_outputs >= 0.5]\n",
    "\n",
    "    # train_accuracy = metrics.accuracy_score(train_targets, train_outputs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # train_mcc = metrics.matthews_corrcoef(train_targets, train_outputs)\n",
    "    # print(f\"Train Accuracy Score: {train_accuracy}\")\n",
    "    # wandb.log({'Train Accuracy Score': train_accuracy, 'Epoch': epoch})\n",
    "    # print(f\"Train MCC Score: {train_mcc}\")\n",
    "    # wandb.log({'Train MCC Score': train_mcc, 'Epoch': epoch})\n",
    "\n",
    "    # valid_outputs =valid_outputs[valid_outputs >= 0.5]\n",
    "\n",
    "    # accuracy = metrics.accuracy_score(valid_targets, valid_outputs)\n",
    "    # valid_mcc = metrics.matthews_corrcoef(valid_targets, valid_outputs)\n",
    "    # print(f\"Valid Accuracy Score: {accuracy}\")\n",
    "    # wandb.log({'Valid Accuracy Score': accuracy, 'Epoch': epoch})\n",
    "    # print(f\"Valid MCC Score: {valid_mcc}\")\n",
    "    # wandb.log({'Valid MCC Score': valid_mcc, 'Epoch': epoch})\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False,  True, ...,  True, False,  True])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "541dbd94202b20b79e7450b85e048bbe171c76f998cc8290e5c44032e35da434"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
